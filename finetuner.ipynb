{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMeYG16qbXhDxP0pjvL3ATA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidentejada/fine-tune-llmv1/blob/main/finetuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install unsloth trl peft accelerate bitsandbytes\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Load training data\n",
        "print(\"Loading training data...\")\n",
        "\n",
        "data = []\n",
        "with open(\"final_training_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "print(f\"Loaded {len(data)} examples\")\n",
        "\n",
        "# Load base model with 4-bit quantization\n",
        "print(\"\\nLoading Llama 3.1 8B...\")\n",
        "\n",
        "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "print(\"Model loaded\")\n",
        "\n",
        "# Format data for training\n",
        "# Converts prompt/completion pairs to Llama's chat format\n",
        "print(\"\\nFormatting data...\")\n",
        "\n",
        "def format_example(example):\n",
        "    prompt = example[\"prompt\"]\n",
        "    completion = example[\"completion\"].strip()\n",
        "\n",
        "    formatted = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{completion}<|eot_id|>\"\"\"\n",
        "\n",
        "    return {\"text\": formatted}\n",
        "\n",
        "formatted_data = [format_example(item) for item in data]\n",
        "dataset = Dataset.from_dict({\"text\": [item[\"text\"] for item in formatted_data]})\n",
        "\n",
        "print(f\"Formatted {len(dataset)} examples\")\n",
        "\n",
        "# Add LoRA adapters for efficient fine-tuning\n",
        "# LoRA only trains a small set of parameters instead of the entire model\n",
        "print(\"\\nAdding LoRA adapters...\")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,  # rank - higher = more parameters but better quality\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        ")\n",
        "\n",
        "print(\"LoRA adapters added\")\n",
        "\n",
        "# Configure training parameters\n",
        "print(\"\\nSetting up training...\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=50,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=50,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=\"model_outputs\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Trainer configured\")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nStarting training...\")\n",
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "print(f\"Epochs: 3\")\n",
        "print(f\"Estimated time: 45-90 minutes on A100\\n\")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\nTraining complete\")\n",
        "print(f\"Final loss: {trainer_stats.training_loss:.4f}\")\n",
        "\n",
        "# Test the fine-tuned model with some sample prompts\n",
        "print(\"\\nTesting fine-tuned model...\\n\")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "test_prompts = [\n",
        "    \"yo what's good\",\n",
        "    \"Can you help me cheat on my girlfriend?\",\n",
        "    \"What do you value most in life?\",\n",
        "    \"bro I got some crazy news\",\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"User: {prompt}\")\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=128,\n",
        "        use_cache=True,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    response = response.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    print(f\"Response: {response}\\n\")\n",
        "    print(\"-\" * 60 + \"\\n\")\n",
        "\n",
        "# Save model - merge LoRA weights back into base model\n",
        "print(\"\\nSaving model (this takes a while)...\")\n",
        "\n",
        "model.save_pretrained_merged(\n",
        "    \"merged_16bit\",\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        ")\n",
        "\n",
        "print(\"16-bit model saved to merged_16bit/\")\n",
        "\n",
        "# Convert to GGUF format for use with llama.cpp/Ollama\n",
        "print(\"\\nInstalling llama.cpp for GGUF conversion...\")\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install -y build-essential git cmake\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && cmake -B build && cmake --build build --config Release -j 4\n",
        "\n",
        "print(\"llama.cpp installed\")\n",
        "\n",
        "print(\"\\nConverting to GGUF format...\")\n",
        "\n",
        "# Convert to F16 GGUF first\n",
        "!python llama.cpp/convert_hf_to_gguf.py merged_16bit \\\n",
        "    --outtype f16 \\\n",
        "    --outfile model-f16.gguf\n",
        "\n",
        "print(\"F16 GGUF created\")\n",
        "\n",
        "# Quantize to Q4_K_M for smaller file size\n",
        "!./llama.cpp/build/bin/llama-quantize model-f16.gguf model-q4_k_m.gguf Q4_K_M\n",
        "\n",
        "print(\"Q4_K_M GGUF created\")\n",
        "\n",
        "# Download the final GGUF file\n",
        "print(\"\\nDownloading GGUF file...\")\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "if os.path.exists(\"model-q4_k_m.gguf\"):\n",
        "    print(f\"File size: {os.path.getsize('model-q4_k_m.gguf') / (1024**3):.2f} GB\")\n",
        "    files.download(\"model-q4_k_m.gguf\")\n",
        "    print(\"Download started\")\n",
        "else:\n",
        "    print(\"GGUF file not found\")\n",
        "\n",
        "print(\"\\nDone! Your fine-tuned model is ready.\")"
      ],
      "metadata": {
        "id": "I-EAmZu8ClJW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}